{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD1BgPzA/1e6Dow/Yz1y9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush24578/Concepts-and-Technologies-of-AI/blob/main/Ayush_Dhungana_Worksheet8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V9nuNkeWcw-U"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class CustomDecisionTree:\n",
        "  def __init__(self, max_depth=None):\n",
        "    self.max_depth = max_depth\n",
        "    self.tree = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.tree = self._build_tree(X, y)\n",
        "\n",
        "  def _build_tree(self, X, y, depth=0):\n",
        "    num_samples, num_features = X.shape\n",
        "    unique_classes = np.unique(y)\n",
        "\n",
        "    # Stopping conditions: pure node or reached max depth\n",
        "    if len(unique_classes) == 1:\n",
        "      return {'class': unique_classes[0]}\n",
        "\n",
        "    if num_samples == 0 or (self.max_depth and depth >= self.max_depth):\n",
        "      return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "    best_info_gain = -float('inf')\n",
        "    best_split = None\n",
        "\n",
        "    for feature_idx in range(num_features):\n",
        "      thresholds = np.unique(X[:, feature_idx])\n",
        "\n",
        "      for threshold in thresholds:\n",
        "        left_mask = X[:, feature_idx] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "\n",
        "        left_y = y[left_mask]\n",
        "        right_y = y[right_mask]\n",
        "\n",
        "        if len(left_y) == 0 or len(right_y) == 0:\n",
        "          continue\n",
        "\n",
        "        info_gain = self._information_gain(y, left_y, right_y)\n",
        "\n",
        "        if info_gain > best_info_gain:\n",
        "          best_info_gain = info_gain\n",
        "          best_split = {\n",
        "            'feature_idx': feature_idx,\n",
        "            'threshold': threshold,\n",
        "            'left_mask': left_mask,\n",
        "            'right_mask': right_mask\n",
        "          }\n",
        "\n",
        "    if best_split is None:\n",
        "      return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "    left_tree = self._build_tree(\n",
        "      X[best_split['left_mask']],\n",
        "      y[best_split['left_mask']],\n",
        "      depth + 1\n",
        "    )\n",
        "\n",
        "    right_tree = self._build_tree(\n",
        "      X[best_split['right_mask']],\n",
        "      y[best_split['right_mask']],\n",
        "      depth + 1\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'feature_idx': best_split['feature_idx'],\n",
        "      'threshold': best_split['threshold'],\n",
        "      'left_tree': left_tree,\n",
        "      'right_tree': right_tree\n",
        "    }\n",
        "\n",
        "  def _information_gain(self, parent, left, right):\n",
        "    parent_entropy = self._entropy(parent)\n",
        "    left_entropy = self._entropy(left)\n",
        "    right_entropy = self._entropy(right)\n",
        "\n",
        "    weighted_avg_entropy = (\n",
        "      (len(left) / len(parent)) * left_entropy +\n",
        "      (len(right) / len(parent)) * right_entropy\n",
        "    )\n",
        "\n",
        "    return parent_entropy - weighted_avg_entropy\n",
        "\n",
        "  def _entropy(self, y):\n",
        "    if len(y) == 0:\n",
        "      return 0\n",
        "    class_probs = np.bincount(y) / len(y)\n",
        "    return -np.sum(class_probs * np.log2(class_probs + 1e-9))\n",
        "\n",
        "  def predict(self, X):\n",
        "    return [self._predict_single(x, self.tree) for x in X]\n",
        "\n",
        "  def _predict_single(self, x, tree):\n",
        "    if 'class' in tree:\n",
        "      return tree['class']\n",
        "\n",
        "    feature_val = x[tree['feature_idx']]\n",
        "\n",
        "    if feature_val <= tree['threshold']:\n",
        "      return self._predict_single(x, tree['left_tree'])\n",
        "    else:\n",
        "      return self._predict_single(x, tree['right_tree'])\n"
      ],
      "metadata": {
        "id": "K9TkAKnodYws"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "# Split into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "oNnG80L1on36"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the custom decision tree\n",
        "custom_tree = CustomDecisionTree(max_depth=3)\n",
        "custom_tree.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred_custom = custom_tree.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Custom Decision Tree Accuracy: {accuracy_custom:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIFu5OZ2or7W",
        "outputId": "195738b1-23ed-4535-e2cc-f8a38863a0e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Scikit-learn decision tree\n",
        "sklearn_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sklearn_tree.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"Scikit-learn Decision Tree Accuracy: {accuracy_sklearn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GvNXFkdowqE",
        "outputId": "7f942c13-e2eb-4a79-f64b-9c6fa9039c02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scikit-learn Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy Comparison:\")\n",
        "print(f\"Custom Decision Tree: {accuracy_custom:.4f}\")\n",
        "print(f\"Scikit-learn Decision Tree: {accuracy_sklearn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJh0HpBAox2G",
        "outputId": "6a46a17b-683b-4b3b-ff42-753e2d202fb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison:\n",
            "Custom Decision Tree: 1.0000\n",
            "Scikit-learn Decision Tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import f1_score, mean_squared_error\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load Wine dataset\n",
        "wine_data = load_wine()\n",
        "X_wine = wine_data.data\n",
        "y_wine = wine_data.target\n",
        "\n",
        "\n",
        "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
        "    X_wine, y_wine, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "dt_classifier.fit(X_train_w, y_train_w)\n",
        "rf_classifier.fit(X_train_w, y_train_w)\n",
        "\n",
        "\n",
        "y_pred_dt = dt_classifier.predict(X_test_w)\n",
        "y_pred_rf = rf_classifier.predict(X_test_w)\n",
        "\n",
        "f1_dt = f1_score(y_test_w, y_pred_dt, average='weighted')\n",
        "f1_rf = f1_score(y_test_w, y_pred_rf, average='weighted')\n",
        "\n",
        "print(\"=== Classification Results ===\")\n",
        "print(f\"Decision Tree F1 Score: {f1_dt:.4f}\")\n",
        "print(f\"Random Forest F1 Score: {f1_rf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s2roULSo1S5",
        "outputId": "c86952fe-a70c-4655-f182-1f7a7f020c76"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Classification Results ===\n",
            "Decision Tree F1 Score: 0.9440\n",
            "Random Forest F1 Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Hyperparameter Tuning with GridSearchCV\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid_rf,\n",
        "    cv=5,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_w, y_train_w)\n",
        "\n",
        "# Results\n",
        "print(\"\\n=== GridSearchCV Results ===\")\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation F1 Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred_best = best_rf.predict(X_test_w)\n",
        "f1_best = f1_score(y_test_w, y_pred_best, average='weighted')\n",
        "print(f\"Test Set F1 Score with Best Model: {f1_best:.4f}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGlgI_2Go5nm",
        "outputId": "24534a42-0666-40be-ccb4-95a67d81eb91"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== GridSearchCV Results ===\n",
            "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
            "Best Cross-Validation F1 Score: 0.9783\n",
            "Test Set F1 Score with Best Model: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load regression dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "# Load diabetes dataset for regression\n",
        "diabetes = load_diabetes()\n",
        "X_reg = diabetes.data\n",
        "y_reg = diabetes.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree and Random Forest Regressors\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Fit models\n",
        "dt_regressor.fit(X_train_r, y_train_r)\n",
        "rf_regressor.fit(X_train_r, y_train_r)\n",
        "\n",
        "\n",
        "y_pred_dt_r = dt_regressor.predict(X_test_r)\n",
        "y_pred_rf_r = rf_regressor.predict(X_test_r)\n",
        "\n",
        "mse_dt = mean_squared_error(y_test_r, y_pred_dt_r)\n",
        "mse_rf = mean_squared_error(y_test_r, y_pred_rf_r)\n",
        "\n",
        "print(\"\\n=== Regression Results ===\")\n",
        "print(f\"Decision Tree Regressor MSE: {mse_dt:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
        "\n",
        "# 4. Hyperparameter Tuning for Random Forest Regressor (RandomizedSearchCV)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist_rf_reg = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=42),\n",
        "    param_distributions=param_dist_rf_reg,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_r, y_train_r)\n",
        "\n",
        "# Results\n",
        "print(\"\\n=== RandomizedSearchCV Results ===\")\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation MSE: {-random_search.best_score_:.4f}\")\n",
        "\n",
        "\n",
        "best_rf_reg = random_search.best_estimator_\n",
        "y_pred_best_reg = best_rf_reg.predict(X_test_r)\n",
        "mse_best = mean_squared_error(y_test_r, y_pred_best_reg)\n",
        "print(f\"Test Set MSE with Best Model: {mse_best:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CQ5FonQo80S",
        "outputId": "5b2a0cf3-42e2-40e1-872c-d9556537e29c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Regression Results ===\n",
            "Decision Tree Regressor MSE: 4976.7978\n",
            "Random Forest Regressor MSE: 2952.0106\n",
            "\n",
            "=== RandomizedSearchCV Results ===\n",
            "Best Parameters: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}\n",
            "Best Cross-Validation MSE: 3318.2050\n",
            "Test Set MSE with Best Model: 2828.4947\n"
          ]
        }
      ]
    }
  ]
}